---
phase: 03-ai-routing-core
plan: 03
type: execute
wave: 2
depends_on:
  - 03-01-PLAN.md
files_modified:
  - COREDIRECTIVE_ENGINE/workflow_supervisor_agent.json
  - COREDIRECTIVE_ENGINE/sql/routing_metrics.sql
autonomous: true

must_haves:
  truths:
    - "AI Agent has fallback logic for low-confidence routing decisions"
    - "Gibberish inputs return helpful clarification, not errors"
    - "Routing confidence is inferred from AI behavior and logged"
  artifacts:
    - path: "COREDIRECTIVE_ENGINE/workflow_supervisor_agent.json"
      provides: "Fallback handling logic in AI prompt"
      contains: "ROUTING RULES"
    - path: "COREDIRECTIVE_ENGINE/sql/routing_metrics.sql"
      provides: "SQL for routing analytics"
      contains: "routing_decision"
  key_links:
    - from: "AI Agent system prompt"
      to: "Fallback response pattern"
      via: "ROUTING RULES section"
      pattern: "70% confident"
---

<objective>
Implement confidence threshold logic and graceful fallback handling for ambiguous or unclear user inputs.

Purpose: Enable the system to degrade gracefully (SC-3.3) when routing confidence is low, and log confidence scores for monitoring (SC-3.4).

Output: AI Agent that asks clarifying questions or provides helpful responses when uncertain, instead of failing or guessing.
</objective>

<execution_context>
@/Users/et/.claude/get-shit-done/workflows/execute-plan.md
@/Users/et/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/03-ai-routing-core/03-01-PLAN.md
@COREDIRECTIVE_ENGINE/workflow_supervisor_agent.json
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add confidence threshold and fallback logic to AI Agent prompt</name>
  <files>COREDIRECTIVE_ENGINE/workflow_supervisor_agent.json</files>
  <action>
    Enhance the AI Agent system prompt with explicit fallback handling rules:

    1. Locate the "ROUTING RULES" section added in 03-01-PLAN.md

    2. Add detailed fallback behavior guidelines:
       ```
       ## FALLBACK HANDLING

       ### Confidence Levels (Self-Assessment)
       - **HIGH (>80%)**: Clear intent, known tool, specific request ‚Üí Execute immediately
       - **MEDIUM (50-80%)**: General topic match, some ambiguity ‚Üí Ask ONE clarifying question
       - **LOW (<50%)**: Unclear, gibberish, or completely unknown ‚Üí Provide guidance

       ### Fallback Response Patterns

       **For Medium Confidence:**
       - "I can help with [X] or [Y]. Which would you prefer?"
       - "Did you mean [most likely interpretation]?"
       - "I can [tool A] or [tool B]. What are you trying to do?"

       **For Low Confidence (Gibberish/Unknown):**
       - Don't say: "I don't understand" ‚ùå
       - Do say: "I can help with: **System status**, **Task selection**, or **Finance tracking**. What do you need?" ‚úÖ
       - Show available capabilities as options
       - Be helpful, not dismissive

       ### Special Cases
       - Greetings (hi, hello, hey) ‚Üí Friendly response + "What can I help with?"
       - Thanks/acknowledgment ‚Üí Brief acknowledgment, no tool call needed
       - Empty/very short input ‚Üí "What would you like to do?"
       - Profanity/frustration ‚Üí Empathetic response + capability list

       ### Never Do This
       - ‚ùå "I can't do that"
       - ‚ùå "That's not in my capabilities"
       - ‚ùå "Error: unknown command"
       - ‚ùå Suggest tools that don't exist

       ### Always Do This
       - ‚úÖ Acknowledge the user's need
       - ‚úÖ Suggest the closest matching tool
       - ‚úÖ Keep response under 200 words (ADHD-friendly)
       - ‚úÖ End with clear next action
       ```

    3. Add examples of fallback responses:
       ```
       ## FALLBACK EXAMPLES

       Input: "asdfghjkl"
       Output: "I can help with: **System status**, **Focus task selection**, or **Finance tracking**. What do you need?"

       Input: "idk what to do"
       Output: "Sounds like decision paralysis. I can pull your highest-priority task from Notion. Want me to do that?"

       Input: "everything is broken"
       Output: "Let me check system health for you. Give me a sec..."
       [Calls System_Status tool]

       Input: "yo"
       Output: "Hey! What can I help with today?"
       ```

    4. Ensure temperature stays at 0.4 to maintain consistent routing behavior

    This provides the AI with clear guidelines for handling edge cases and ambiguous inputs.
  </action>
  <verify>
    # Check for fallback handling section
    grep "FALLBACK HANDLING" COREDIRECTIVE_ENGINE/workflow_supervisor_agent.json

    # Verify confidence levels defined
    grep -E "HIGH.*80%|MEDIUM.*50-80%|LOW.*50%" COREDIRECTIVE_ENGINE/workflow_supervisor_agent.json

    # Check for fallback examples
    grep "asdfghjkl" COREDIRECTIVE_ENGINE/workflow_supervisor_agent.json
  </verify>
  <done>
    AI Agent system prompt includes confidence thresholds and detailed fallback handling patterns with examples.
  </done>
</task>

<task type="auto">
  <name>Task 2: Enhance routing decision logging with confidence indicators</name>
  <files>COREDIRECTIVE_ENGINE/workflow_supervisor_agent.json</files>
  <action>
    Improve the "Log Routing Decision" node (added in 03-01) to better infer confidence:

    1. Locate the Code node with id "log-routing-decision"

    2. Enhance confidence estimation logic:
       ```javascript
       // Log AI routing decision with confidence estimation
       const agent = $input.first().json;
       const timestamp = new Date().toISOString();
       const executionId = $execution.id || 'N/A';
       const ctx = $('Parse Input').first().json;

       // Extract tool calls
       const toolCalls = agent.intermediate_steps || [];
       const toolNames = toolCalls.map(step => step.tool || 'unknown');
       const output = agent.output || '';

       // Improved confidence estimation based on multiple signals
       let confidence = 'UNKNOWN';
       let confidenceScore = 0;

       // Signal 1: Tool was called (high confidence)
       if (toolNames.length > 0) {
         confidenceScore += 0.7;
       }

       // Signal 2: Response is decisive (not a question)
       if (!output.includes('?') && !output.includes('which') && !output.includes('prefer')) {
         confidenceScore += 0.2;
       }

       // Signal 3: Response length (longer = more confident)
       if (output.length > 100) {
         confidenceScore += 0.1;
       }

       // Map score to label
       if (confidenceScore >= 0.8) confidence = 'HIGH';
       else if (confidenceScore >= 0.5) confidence = 'MEDIUM';
       else confidence = 'LOW';

       // Detect fallback responses
       const isFallback = output.includes('can help with') ||
                          output.includes('What do you need') ||
                          output.includes('Which would you prefer');

       const logEntry = {
         event: 'routing_decision',
         timestamp,
         executionId,
         user_input: ctx.text?.substring(0, 100),
         tools_called: toolNames,
         confidence_label: confidence,
         confidence_score: Math.round(confidenceScore * 100) / 100,
         is_fallback: isFallback,
         response_length: output.length,
         has_tool_output: toolNames.length > 0,
         response_preview: output.substring(0, 150)
       };

       console.log('ROUTING_DECISION:', JSON.stringify(logEntry));

       // Pass through original agent output
       return { json: agent };
       ```

    3. This enhanced logging enables:
       - SC-3.4: Routing decision logged with confidence score
       - Monitoring of fallback rate
       - Input/output correlation for debugging

    4. Verify the node is still connected correctly:
       - Input: Supervisor Agent main output
       - Output: Format Output node
  </action>
  <verify>
    # Check for enhanced logging code
    grep "confidence_score" COREDIRECTIVE_ENGINE/workflow_supervisor_agent.json

    # Verify fallback detection logic
    grep "isFallback" COREDIRECTIVE_ENGINE/workflow_supervisor_agent.json

    # Check connection still exists
    grep -A3 '"Log Routing Decision"' COREDIRECTIVE_ENGINE/workflow_supervisor_agent.json
  </verify>
  <done>
    Routing decision logging enhanced with multi-signal confidence estimation and fallback detection.
  </done>
</task>

<task type="auto">
  <name>Task 3: Create PostgreSQL table for routing metrics</name>
  <files>COREDIRECTIVE_ENGINE/sql/routing_metrics.sql</files>
  <action>
    Create SQL schema for storing routing analytics (optional enhancement for monitoring):

    1. Create file: `COREDIRECTIVE_ENGINE/sql/routing_metrics.sql`

    2. Define routing metrics table:
       ```sql
       -- Routing Metrics Table
       -- Stores AI routing decisions for analytics and optimization
       -- Created: Phase 03-03

       CREATE TABLE IF NOT EXISTS routing_metrics (
         id SERIAL PRIMARY KEY,
         execution_id VARCHAR(255),
         chat_id BIGINT,
         user_input TEXT,
         tools_called TEXT[], -- Array of tool names
         confidence_label VARCHAR(20), -- HIGH, MEDIUM, LOW
         confidence_score DECIMAL(3, 2), -- 0.00 to 1.00
         is_fallback BOOLEAN DEFAULT false,
         response_length INTEGER,
         latency_ms INTEGER,
         created_at TIMESTAMP DEFAULT NOW()
       );

       -- Index for performance
       CREATE INDEX IF NOT EXISTS idx_routing_chat_id ON routing_metrics(chat_id);
       CREATE INDEX IF NOT EXISTS idx_routing_confidence ON routing_metrics(confidence_label);
       CREATE INDEX IF NOT EXISTS idx_routing_created ON routing_metrics(created_at);

       -- Analytics view: Routing accuracy by confidence level
       CREATE OR REPLACE VIEW routing_accuracy AS
       SELECT
         confidence_label,
         COUNT(*) as total_routes,
         SUM(CASE WHEN is_fallback THEN 1 ELSE 0 END) as fallback_count,
         ROUND(AVG(confidence_score)::numeric, 2) as avg_confidence,
         ROUND(AVG(latency_ms)::numeric, 0) as avg_latency_ms
       FROM routing_metrics
       GROUP BY confidence_label
       ORDER BY confidence_label;

       -- Analytics view: Tool usage frequency
       CREATE OR REPLACE VIEW tool_usage_stats AS
       SELECT
         UNNEST(tools_called) as tool_name,
         COUNT(*) as call_count,
         ROUND(AVG(latency_ms)::numeric, 0) as avg_latency_ms
       FROM routing_metrics
       WHERE tools_called IS NOT NULL AND array_length(tools_called, 1) > 0
       GROUP BY tool_name
       ORDER BY call_count DESC;

       COMMENT ON TABLE routing_metrics IS 'Tracks AI routing decisions for monitoring and optimization';
       COMMENT ON VIEW routing_accuracy IS 'Shows routing performance by confidence level';
       COMMENT ON VIEW tool_usage_stats IS 'Shows which tools are called most frequently';
       ```

    3. Add deployment note to 03-02-DEPLOYMENT.md:
       - This table is optional for Phase 3
       - Can be added later for advanced analytics
       - Current console.log() logging is sufficient for initial testing

    Note: This table is not required for Phase 3 success criteria, but provides foundation for future optimization.
  </action>
  <verify>
    # Check SQL file exists
    test -f COREDIRECTIVE_ENGINE/sql/routing_metrics.sql && echo "SQL_EXISTS"

    # Verify table creation
    grep "CREATE TABLE.*routing_metrics" COREDIRECTIVE_ENGINE/sql/routing_metrics.sql

    # Check for analytics views
    grep -c "CREATE.*VIEW" COREDIRECTIVE_ENGINE/sql/routing_metrics.sql
    # Should return 2 (routing_accuracy, tool_usage_stats)
  </verify>
  <done>
    SQL schema created for routing metrics with analytics views. Marked as optional enhancement.
  </done>
</task>

<task type="auto">
  <name>Task 4: Create fallback handling test cases</name>
  <files>.planning/phases/03-ai-routing-core/03-03-TEST-CASES.md</files>
  <action>
    Create additional test cases specifically for fallback and edge case handling:

    1. Create file: `.planning/phases/03-ai-routing-core/03-03-TEST-CASES.md`

    2. Define fallback-focused test scenarios:
       ```markdown
       # Phase 03-03 Fallback Handling Test Cases

       ## Success Criteria Focus: SC-3.3

       ### Gibberish Inputs (Should NOT error)
       - TC-1: "asdfghjkl" ‚Üí Helpful response listing capabilities
       - TC-2: "12345 banana robot" ‚Üí Clarification or capability list
       - TC-3: "............." ‚Üí Prompt for valid input
       - TC-4: "???" ‚Üí "What can I help with?"

       ### Ambiguous Inputs (Should ask for clarification)
       - TC-5: "status" ‚Üí Clarify: system status or task status?
       - TC-6: "help" ‚Üí List available capabilities
       - TC-7: "do something" ‚Üí Ask what specifically
       - TC-8: "idk" ‚Üí Suggest ADHD Commander (decision paralysis)

       ### Edge Cases
       - TC-9: Empty message ‚Üí "What would you like to do?"
       - TC-10: Only emoji "ü§î" ‚Üí Friendly response + capability list
       - TC-11: "fuck this" ‚Üí Empathetic + "What's frustrating you?"
       - TC-12: Very long rambling input (200+ words) ‚Üí Extract intent or ask to clarify

       ### Social Inputs (No tool call needed)
       - TC-13: "hello" ‚Üí Greeting + "What can I help with?"
       - TC-14: "thanks" ‚Üí "You're welcome!" (no tool)
       - TC-15: "good morning" ‚Üí Time-aware greeting
       - TC-16: "see you later" ‚Üí "Take care!"

       ## Expected Behavior Patterns

       ### HIGH Confidence (Tool Call)
       - Clear intent ‚Üí Immediate tool execution
       - Log: `confidence_label: 'HIGH'`, `is_fallback: false`

       ### MEDIUM Confidence (Clarification)
       - Partial match ‚Üí ONE clarifying question
       - Log: `confidence_label: 'MEDIUM'`, `is_fallback: false`

       ### LOW Confidence (Capability List)
       - No match ‚Üí Helpful response with options
       - Log: `confidence_label: 'LOW'`, `is_fallback: true`

       ## Anti-Patterns to Avoid

       ‚ùå "I don't understand"
       ‚ùå "Error: invalid input"
       ‚ùå "Command not recognized"
       ‚ùå Silence/no response
       ‚ùå Stack trace or technical error

       ‚úÖ "I can help with..."
       ‚úÖ "Did you mean...?"
       ‚úÖ "What are you trying to do?"
       ‚úÖ Friendly acknowledgment

       ## Manual Testing Procedure

       1. Deploy updated workflow with fallback handling
       2. Send each test case via Telegram
       3. Record actual response
       4. Check logs for:
          - `ROUTING_DECISION` entry
          - `confidence_label` and `confidence_score`
          - `is_fallback` flag
       5. Verify no errors or crashes

       ## Pass Criteria

       - 14/16 test cases handled gracefully (no errors)
       - Fallback responses are helpful, not dismissive
       - Confidence scores logged accurately
       - Average response time <5 seconds
       ```

    3. These test cases complement 03-01-TEST-CASES.md
       - 03-01 focuses on ROUTING accuracy
       - 03-03 focuses on FALLBACK handling

    4. Both will be used in 03-04-PLAN.md for comprehensive testing
  </action>
  <verify>
    test -f .planning/phases/03-ai-routing-core/03-03-TEST-CASES.md && echo "FALLBACK_TESTS_EXIST"

    # Check for gibberish test cases
    grep "asdfghjkl" .planning/phases/03-ai-routing-core/03-03-TEST-CASES.md

    # Verify anti-patterns section
    grep "Anti-Patterns" .planning/phases/03-ai-routing-core/03-03-TEST-CASES.md
  </verify>
  <done>
    Fallback handling test cases created with 16 edge case scenarios and clear pass/fail criteria.
  </done>
</task>

</tasks>

<verification>
1. AI Agent system prompt includes FALLBACK HANDLING section with confidence thresholds
2. Routing decision logging enhanced with multi-signal confidence estimation
3. SQL schema created for optional routing analytics
4. Fallback handling test cases defined for graceful degradation validation
</verification>

<success_criteria>
- SC-3.3 IMPLEMENTED: Fallback logic for gibberish/unclear inputs returns helpful guidance
- SC-3.4 ENHANCED: Confidence score calculation improved with multiple signals
- Fallback responses follow ADHD-friendly format (brief, actionable)
</success_criteria>

<output>
After completion, create `.planning/phases/03-ai-routing-core/03-03-SUMMARY.md`
</output>
