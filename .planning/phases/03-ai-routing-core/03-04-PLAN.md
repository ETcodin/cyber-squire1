---
phase: 03-ai-routing-core
plan: 04
type: checkpoint
wave: 3
depends_on:
  - 03-01-PLAN.md
  - 03-02-PLAN.md
  - 03-03-PLAN.md
files_modified:
  - .planning/phases/03-ai-routing-core/03-04-RESULTS.md
  - COREDIRECTIVE_ENGINE/workflow_supervisor_agent.json
autonomous: false

must_haves:
  truths:
    - "All Phase 3 test cases executed and results documented"
    - "Routing latency measured and logged"
    - "Success criteria SC-3.1 through SC-3.5 validated"
  artifacts:
    - path: ".planning/phases/03-ai-routing-core/03-04-RESULTS.md"
      provides: "Test execution results and metrics"
      contains: "SC-3"
    - path: "COREDIRECTIVE_ENGINE/workflow_supervisor_agent.json"
      provides: "Final production-ready routing workflow"
      contains: "AI Routing Core v1"
  key_links:
    - from: "Test cases"
      to: "Success criteria"
      via: "Validation mapping"
      pattern: "SC-3\\.[1-5]"
---

<objective>
Execute comprehensive testing of the AI Routing Core, measure performance, and validate all Phase 3 success criteria before proceeding to Phase 4.

Purpose: Ensure routing works reliably with natural language inputs, handles edge cases gracefully, and meets latency requirements (SC-3.5: <3 seconds average).

Output: Test results document with pass/fail status for all success criteria and performance metrics.
</objective>

<execution_context>
@/Users/et/.claude/get-shit-done/workflows/execute-plan.md
@/Users/et/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/03-ai-routing-core/03-01-PLAN.md
@.planning/phases/03-ai-routing-core/03-02-PLAN.md
@.planning/phases/03-ai-routing-core/03-03-PLAN.md
@.planning/phases/03-ai-routing-core/03-01-TEST-CASES.md
@.planning/phases/03-ai-routing-core/03-03-TEST-CASES.md
@COREDIRECTIVE_ENGINE/workflow_supervisor_agent.json
</context>

<tasks>

<task type="manual">
  <name>Task 1: Deploy Phase 3 workflows to production</name>
  <files>None</files>
  <action>
    Deploy all Phase 3 updates to the EC2 instance for live testing:

    1. Transfer updated workflows to EC2:
       ```bash
       scp -i ~/.ssh/cyber-squire-key.pem \
           COREDIRECTIVE_ENGINE/workflow_supervisor_agent.json \
           COREDIRECTIVE_ENGINE/workflow_tool_system_status.json \
           ec2-user@54.234.155.244:/tmp/
       ```

    2. Follow deployment guide from 03-02-DEPLOYMENT.md:
       - Import workflow_tool_system_status.json first
       - Note the workflow ID
       - Update workflow_supervisor_agent.json with correct workflow ID
       - Re-import updated Supervisor Agent
       - Activate both workflows

    3. Verify deployment:
       - Check n8n UI shows all tool connections (3 tools: ADHD, Finance, System Status)
       - Inspect AI Agent node to confirm tools are wired
       - Send test message: "ping" → Should get response (basic connectivity)

    4. Access n8n logs to monitor ROUTING_DECISION entries:
       ```bash
       ssh -i ~/.ssh/cyber-squire-key.pem ec2-user@54.234.155.244
       docker logs -f cd-service-n8n 2>&1 | grep "ROUTING_DECISION"
       ```

    This is a MANUAL task - requires human interaction with n8n UI.
  </action>
  <verify>
    # Verify workflows transferred
    ssh -i ~/.ssh/cyber-squire-key.pem ec2-user@54.234.155.244 \
      'ls -la /tmp/workflow_supervisor_agent.json /tmp/workflow_tool_system_status.json'

    # Check n8n is running
    ssh -i ~/.ssh/cyber-squire-key.pem ec2-user@54.234.155.244 \
      'docker ps | grep n8n'
  </verify>
  <done>
    Phase 3 workflows deployed to production n8n. Tools visible in AI Agent node. Ready for testing.
  </done>
</task>

<task type="manual">
  <name>Task 2: Execute routing accuracy test suite (SC-3.1, SC-3.2, SC-3.3)</name>
  <files>.planning/phases/03-ai-routing-core/03-04-RESULTS.md</files>
  <action>
    Run all test cases from 03-01-TEST-CASES.md and 03-03-TEST-CASES.md:

    1. Create results file: `.planning/phases/03-ai-routing-core/03-04-RESULTS.md`

    2. Test routing accuracy (from 03-01-TEST-CASES.md):
       - Send each TC-1 through TC-10 via Telegram bot
       - Record actual response and tool called (check n8n execution history)
       - Compare with expected behavior

    3. Test fallback handling (from 03-03-TEST-CASES.md):
       - Send TC-1 through TC-16 via Telegram
       - Verify no errors or crashes
       - Check responses are helpful and ADHD-formatted

    4. Document results in markdown table:
       ```markdown
       # Phase 03 Test Results

       Executed: [DATE]
       Tester: [NAME]
       Environment: Production (54.234.155.244)

       ## Routing Accuracy Tests (03-01)

       | Test | Input | Expected | Actual | Tool Called | Pass/Fail |
       |------|-------|----------|--------|-------------|-----------|
       | TC-1 | "Check system health" | System_Status | [ACTUAL] | [TOOL] | ✅/❌ |
       | TC-2 | "Is the server okay?" | System_Status | [ACTUAL] | [TOOL] | ✅/❌ |
       | ... | ... | ... | ... | ... | ... |

       ## Fallback Handling Tests (03-03)

       | Test | Input | Expected | Actual | Confidence | Pass/Fail |
       |------|-------|----------|--------|------------|-----------|
       | TC-1 | "asdfghjkl" | Helpful guidance | [ACTUAL] | LOW | ✅/❌ |
       | TC-2 | "12345 banana" | Capability list | [ACTUAL] | LOW | ✅/❌ |
       | ... | ... | ... | ... | ... | ... |

       ## Summary
       - Routing Tests: X/10 passed
       - Fallback Tests: X/16 passed
       - Total: X/26 passed (X% success rate)
       ```

    5. Passing threshold: 18/20 routing tests, 14/16 fallback tests (as per roadmap)

    This is MANUAL testing - requires sending Telegram messages and recording results.
  </action>
  <verify>
    test -f .planning/phases/03-ai-routing-core/03-04-RESULTS.md && echo "RESULTS_FILE_EXISTS"

    # Check for results table
    grep "Pass/Fail" .planning/phases/03-ai-routing-core/03-04-RESULTS.md
  </verify>
  <done>
    All 26 test cases executed. Results documented with pass/fail status and actual outputs.
  </done>
</task>

<task type="manual">
  <name>Task 3: Measure routing latency (SC-3.5)</name>
  <files>.planning/phases/03-ai-routing-core/03-04-RESULTS.md</files>
  <action>
    Measure end-to-end routing latency to validate SC-3.5 (<3 seconds average):

    1. Extract latency from n8n logs:
       ```bash
       ssh -i ~/.ssh/cyber-squire-key.pem ec2-user@54.234.155.244 << 'EOF'
       docker logs cd-service-n8n 2>&1 | \
         grep "ROUTING_DECISION" | \
         tail -20 | \
         grep -oP '"latencyMs":\s*\K[0-9]+' | \
         awk '{sum+=$1; count++} END {print "Average:", sum/count, "ms"}'
       EOF
       ```

    2. Alternatively, manually record latency for 10 test messages:
       - Note Telegram message timestamp
       - Note response received timestamp
       - Calculate delta in seconds
       - Average across 10 samples

    3. Add latency results to 03-04-RESULTS.md:
       ```markdown
       ## Latency Metrics (SC-3.5)

       | Sample | Input | Latency (ms) | Within Target (<3s) |
       |--------|-------|--------------|---------------------|
       | 1 | "Check system health" | 2450 | ✅ |
       | 2 | "What should I work on?" | 1820 | ✅ |
       | ... | ... | ... | ... |

       **Average Latency:** X ms (X.X seconds)
       **Target:** <3000 ms (3 seconds)
       **Status:** ✅ PASS / ❌ FAIL
       ```

    4. If latency exceeds 3 seconds average:
       - Check Ollama cold start (verify KEEP_ALIVE=24h from Phase 1)
       - Review tool workflow complexity
       - Consider reducing context window if needed

    This validates SC-3.5 performance requirement.
  </action>
  <verify>
    # Check for latency section in results
    grep "Latency Metrics" .planning/phases/03-ai-routing-core/03-04-RESULTS.md

    # Verify average calculated
    grep "Average Latency" .planning/phases/03-ai-routing-core/03-04-RESULTS.md
  </verify>
  <done>
    Routing latency measured across 10+ samples. Average latency documented and compared to 3-second target.
  </done>
</task>

<task type="auto">
  <name>Task 4: Validate success criteria and create completion report</name>
  <files>.planning/phases/03-ai-routing-core/03-04-RESULTS.md</files>
  <action>
    Review test results and validate all Phase 3 success criteria:

    1. Append success criteria validation to 03-04-RESULTS.md:
       ```markdown
       ## Success Criteria Validation

       ### SC-3.1: "Check system health" routes to status tool (not keyword-matched)
       - Test Cases: TC-1, TC-2, TC-3 (from 03-01)
       - Result: X/3 passed
       - Natural language understanding: YES / NO
       - Status: ✅ PASS / ❌ FAIL

       ### SC-3.2: "What's on my plate today" routes to ADHD Commander
       - Test Cases: TC-4, TC-5, TC-6 (from 03-01)
       - Result: X/3 passed
       - Tool routing accuracy: XX%
       - Status: ✅ PASS / ❌ FAIL

       ### SC-3.3: Gibberish input returns "I didn't understand" (graceful degradation)
       - Test Cases: TC-1, TC-2, TC-3, TC-4 (from 03-03)
       - Result: X/4 passed
       - No errors/crashes: YES / NO
       - Helpful responses: YES / NO
       - Status: ✅ PASS / ❌ FAIL

       ### SC-3.4: Routing decision logged with confidence score
       - Log entries found: YES / NO
       - Confidence scores present: YES / NO
       - Sample log entry:
       ```json
       {
         "event": "routing_decision",
         "confidence_label": "HIGH",
         "confidence_score": 0.87,
         "tools_called": ["System_Status"]
       }
       ```
       - Status: ✅ PASS / ❌ FAIL

       ### SC-3.5: Average routing latency <3 seconds
       - Measured average: X.XX seconds
       - Target: <3 seconds
       - Samples: XX
       - Status: ✅ PASS / ❌ FAIL

       ## Phase 3 Overall Status

       - Success Criteria Passed: X/5
       - Test Cases Passed: X/26
       - Overall Phase Status: ✅ COMPLETE / ⚠️ PARTIAL / ❌ BLOCKED

       ## Recommendations

       - [List any issues found]
       - [Suggest improvements for Phase 4]
       - [Note any deferred items]
       ```

    2. Create executive summary:
       ```markdown
       ## Executive Summary

       Phase 3 (AI Routing Core) delivers intelligent message routing via Ollama qwen2.5:7b with natural language understanding and graceful fallback handling.

       **What Works:**
       - [List passing features]

       **What Needs Work:**
       - [List failing features or improvements needed]

       **Ready for Phase 4:** YES / NO
       - Reasoning: [Brief explanation]
       ```

    3. If all success criteria pass → Phase 3 is COMPLETE
    4. If <4/5 pass → Phase 3 is PARTIAL (requires remediation)
    5. If <2/5 pass → Phase 3 is BLOCKED (cannot proceed)

    This provides clear go/no-go decision for Phase 4.
  </action>
  <verify>
    # Check for success criteria validation
    grep "SC-3\\.[1-5]" .planning/phases/03-ai-routing-core/03-04-RESULTS.md

    # Verify phase status
    grep "Overall Phase Status" .planning/phases/03-ai-routing-core/03-04-RESULTS.md

    # Check for executive summary
    grep "Executive Summary" .planning/phases/03-ai-routing-core/03-04-RESULTS.md
  </verify>
  <done>
    All success criteria validated. Phase 3 completion status determined with supporting evidence. Go/no-go recommendation provided.
  </done>
</task>

<task type="auto">
  <name>Task 5: Update workflow metadata and tag for production</name>
  <files>COREDIRECTIVE_ENGINE/workflow_supervisor_agent.json</files>
  <action>
    Mark the Supervisor Agent workflow as Phase 3 complete:

    1. Update workflow metadata:
       ```json
       "meta": {
         "notes": "Supervisor Agent v3 - AI Routing Core Complete. Phase 3 (2026-02-04). Features: Natural language routing via Ollama qwen2.5:7b, confidence-based fallback handling, routing decision logging. Tools: ADHD Commander, Finance Manager, System Status. Test results: [X/5 SC passed]. Memory: 13-message window. Next: Phase 4 (Context & Memory enhancements).",
         "templateCredsSetupCompleted": true,
         "phase": "03-complete",
         "version": "3.0.0"
       }
       ```

    2. Update tags:
       ```json
       "tags": [
         { "name": "12WY" },
         { "name": "Supervisor" },
         { "name": "Agentic" },
         { "name": "Phase-03" },
         { "name": "Production" }
       ]
       ```

    3. Add version note to system prompt (optional):
       ```
       ---
       CYBER-SQUIRE Routing Core v3.0
       Phase 3 Complete | Natural Language Routing
       ---
       ```

    This documents the production-ready state of the Phase 3 implementation.
  </action>
  <verify>
    # Check for phase metadata
    grep '"phase".*"03-complete"' COREDIRECTIVE_ENGINE/workflow_supervisor_agent.json

    # Verify version tag
    grep '"version".*"3.0.0"' COREDIRECTIVE_ENGINE/workflow_supervisor_agent.json

    # Check tags include Phase-03
    grep "Phase-03" COREDIRECTIVE_ENGINE/workflow_supervisor_agent.json
  </verify>
  <done>
    Workflow metadata updated to reflect Phase 3 completion. Version 3.0.0 tagged for production.
  </done>
</task>

</tasks>

<verification>
1. Phase 3 workflows deployed to production n8n
2. All 26 test cases executed with results documented
3. Routing latency measured and validated against 3-second target
4. Success criteria SC-3.1 through SC-3.5 validated
5. Workflow metadata updated with Phase 3 completion status
</verification>

<success_criteria>
- SC-3.1 VALIDATED: Natural language health check routing tested
- SC-3.2 VALIDATED: ADHD Commander routing tested
- SC-3.3 VALIDATED: Fallback handling tested with gibberish inputs
- SC-3.4 VALIDATED: Routing logs confirmed with confidence scores
- SC-3.5 VALIDATED: Average latency measured and compared to target
</success_criteria>

<output>
After completion, create `.planning/phases/03-ai-routing-core/03-04-SUMMARY.md` with final test results and phase status.
</output>
